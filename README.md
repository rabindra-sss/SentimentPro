# DistilBERT Text Classification
##Overview
This project leverages DistilBERT, a smaller, efficient variant of BERT, for text classification tasks. 
<br> DistilBERT provides a balance between model performance and computational efficiency.

## Features
*Model: DistilBERT (a distilled version of BERT)
*Task: Text classification
*Metrics: Achieved 88.1% accuracy with training and validation losses of 0.0058 and 1.54, respectively.

## Modules
Transformers, Datasets, PyTorch, Evaluate, PEFT, NumPy
