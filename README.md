# SentimentPro: Optimizing Language Model for Sentiment Analysis
## Overview
Fine-tuned a pre-trained transformer-based language model, for sentiment classification on the IMDb dataset.
<br> This project leverages DistilBERT, a smaller, efficient variant of BERT, for text classification tasks. 
<br> DistilBERT provides a balance between model performance and computational efficiency.

## Features
* Model: DistilBERT (a distilled version of BERT)
* Task: Text classification
* Metrics: Achieved 91.5% accuracy with training and validation losses of 0.27 and 0.35, respectively. 

## Modules
Transformers, Datasets, PyTorch, Evaluate, PEFT, NumPy
